{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YhlfU3cqRrH"
      },
      "source": [
        "### **Bonus Question - Connected Components on MapReduce**\n",
        "MapReduce is ideal for network analysis as it enables parallel processing of large graph datasets, making it scalable and efficient. By breaking tasks into map and reduce steps, it allows for distributed analysis of networks, which is essential for handling large-scale graph problems like connected components.\n",
        "\n",
        "1. In this task, you are required to use PySpark and the MapReduce paradigm to identify the connected components in a flight network graph. The focus should be on airports rather than cities. As you know, a connected component refers to a group of airports where every pair of airports within the group is connected either directly or indirectly.\n",
        "\n",
        "The function takes the following inputs:\n",
        "1. Flight network\n",
        "2. A starting date\n",
        "3. An end date\n",
        "\n",
        "The function outputs:\n",
        "1. The number of the connected components during that period\n",
        "2. The size of each connectd componenet\n",
        "3. The airports within the largest connected component identified.\n",
        "\n",
        "__Note:__ For this task, you should check if there is a flight between two airports during that period.\n",
        "__Note:__ You are not allowed to use pre-existing packages or functions in PySpark; instead, you must implement the algorithm from scratch using the MapReduce paradigm.\n",
        "\n",
        "2. Compare the execution time and the results of your implementation with those of the GraphFrames package for identifying connected components. If there is any difference in the results, provide an explanation for why that might occur.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Install all the packages needed using pip**"
      ],
      "metadata": {
        "id": "l3WCSr0j_1hQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFyBcEZ-qRrI"
      },
      "outputs": [],
      "source": [
        "! pip install pyngrok gdown  pyspark  yellowbrick graphframes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Import all the packages needed, included the Spark's one**"
      ],
      "metadata": {
        "id": "3b52HWuu_89j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TpohfDYCqRrJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql.functions import col, explode, collect_list, min as spark_min, struct, size, row_number\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as F\n",
        "from graphframes import GraphFrame\n",
        "import time\n",
        "import json\n",
        "import tempfile\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Initialize the Spark Session**"
      ],
      "metadata": {
        "id": "1DGfLn51APM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark Session with configurations\n",
        "print(\"Initializing Spark Session...\")\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Connected Components Analysis\") \\\n",
        "    .config(\"spark.driver.memory\", \"15g\") \\\n",
        "    .config(\"spark.executor.memory\", \"15g\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
        "    .getOrCreate()\n",
        "print(\"... Spark Session Created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gV2C96Ej5Hem",
        "outputId": "eddb8e34-cb4e-4d0a-ec0a-52d97bc3228c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Spark Session...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Read the CSV file of Airports an print some information**"
      ],
      "metadata": {
        "id": "Va7mKQj2AddD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Reading CSV file...\")\n",
        "df = spark.read.csv(\"/Airports2.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Print basic information about the dataset\n",
        "print(\"\\nDataset Information:\")\n",
        "print(f\"Number of rows: {df.count()}\")\n",
        "print(\"\\nSchema:\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vin5dXEd-9jd",
        "outputId": "968c5a71-0e15-4d09-fdd4-469a10a4626d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading CSV file...\n",
            "\n",
            "Dataset Information:\n",
            "Number of rows: 3606803\n",
            "\n",
            "Schema:\n",
            "root\n",
            " |-- Origin_airport: string (nullable = true)\n",
            " |-- Destination_airport: string (nullable = true)\n",
            " |-- Origin_city: string (nullable = true)\n",
            " |-- Destination_city: string (nullable = true)\n",
            " |-- Passengers: integer (nullable = true)\n",
            " |-- Seats: integer (nullable = true)\n",
            " |-- Flights: integer (nullable = true)\n",
            " |-- Distance: integer (nullable = true)\n",
            " |-- Fly_date: date (nullable = true)\n",
            " |-- Origin_population: integer (nullable = true)\n",
            " |-- Destination_population: integer (nullable = true)\n",
            " |-- Org_airport_lat: string (nullable = true)\n",
            " |-- Org_airport_long: string (nullable = true)\n",
            " |-- Dest_airport_lat: string (nullable = true)\n",
            " |-- Dest_airport_long: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Find out Connected Components using Map Reduce paradigm**"
      ],
      "metadata": {
        "id": "LfXNBcH86ESu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dC8XOTHHqRrL"
      },
      "outputs": [],
      "source": [
        "class ImprovedConnectedComponents:\n",
        "    def __init__(self, max_iterations=20, spark_configs=None):\n",
        "        \"\"\"\n",
        "        Initializes SparkSession with optimized configurations.\n",
        "\n",
        "        Args:\n",
        "            max_iterations (int, optional): Maximum number of iterations for component detection. Defaults to 20.\n",
        "            spark_configs (dict, optional): Additional Spark configurations as key-value pairs.\n",
        "        \"\"\"\n",
        "        print(\"Initializing SparkSession with optimized configurations...\")\n",
        "\n",
        "        builder = SparkSession.builder.appName(\"Optimized Connected Components\")\n",
        "        default_configs = {\n",
        "            \"spark.sql.adaptive.enabled\": \"true\",\n",
        "            \"spark.sql.adaptive.skewJoin.enabled\": \"true\",\n",
        "            \"spark.sql.shuffle.partitions\": \"400\",\n",
        "            \"spark.driver.memory\": \"16g\",\n",
        "            \"spark.executor.memory\": \"16g\",\n",
        "            \"spark.memory.fraction\": \"0.8\",\n",
        "            \"spark.memory.storageFraction\": \"0.2\"\n",
        "        }\n",
        "        # Merge default and custom configurations\n",
        "        for key, value in {**default_configs, **(spark_configs or {})}.items():\n",
        "            builder = builder.config(key, value)\n",
        "\n",
        "        self.spark = builder.getOrCreate()\n",
        "        self.sc = self.spark.sparkContext\n",
        "        self.max_iterations = max_iterations\n",
        "\n",
        "    def find_connected_components(self, flight_network, start_date, end_date):\n",
        "        \"\"\"\n",
        "        Identifies connected components in a flight network using an optimized algorithm.\n",
        "\n",
        "        Args:\n",
        "            flight_network_path (str): Path to the flight network CSV file.\n",
        "            start_date (str): Start date for filtering (format YYYY-MM-DD).\n",
        "            end_date (str): End date for filtering (format YYYY-MM-DD).\n",
        "\n",
        "        Returns:\n",
        "            dict: Results containing connected components information and performance metrics.\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "        print(f\"Analyzing flight network from {start_date} to {end_date}\")\n",
        "        print(f\"Maximum iterations set to: {self.max_iterations}\")\n",
        "\n",
        "        # STEP 1: Load and filter the dataset\n",
        "        print(\"Loading and filtering flight network data...\")\n",
        "        df = flight_network\n",
        "\n",
        "        # Check if necessary columns exist\n",
        "        required_columns = ['Fly_date', 'Origin_airport', 'Destination_airport']\n",
        "        for col_name in required_columns:\n",
        "            if col_name not in df.columns:\n",
        "                raise ValueError(f\"Required column '{col_name}' not found in the dataset\")\n",
        "\n",
        "        # Filter by date range\n",
        "        filtered_df = df.filter((col(\"Fly_date\") >= start_date) & (col(\"Fly_date\") <= end_date))\n",
        "\n",
        "        # Print dataset statistics\n",
        "        total_flights = filtered_df.count()\n",
        "        unique_airports = filtered_df.select(\"Origin_airport\").distinct().count()\n",
        "        print(f\"Total flights in the period: {total_flights}\")\n",
        "        print(f\"Unique airports: {unique_airports}\")\n",
        "\n",
        "        # Create undirected graph edges\n",
        "        edges_df = filtered_df.select(\"Origin_airport\", \"Destination_airport\").distinct()\n",
        "        bidirectional_edges = edges_df.union(\n",
        "            edges_df.select(\n",
        "                col(\"Destination_airport\").alias(\"Origin_airport\"),\n",
        "                col(\"Origin_airport\").alias(\"Destination_airport\")\n",
        "            )\n",
        "        ).distinct()\n",
        "\n",
        "        # STEP 2: Connected Components using Disjoint Set (Union-Find) algorithm\n",
        "        print(\"Computing connected components...\")\n",
        "\n",
        "        # Initial node mapping\n",
        "        initial_nodes = bidirectional_edges.select(\"Origin_airport\").distinct() \\\n",
        "            .withColumnRenamed(\"Origin_airport\", \"airport\") \\\n",
        "            .withColumn(\"component\", col(\"airport\"))\n",
        "\n",
        "        # Iterative component merging\n",
        "        current_nodes = initial_nodes\n",
        "        previous_component_count = 0\n",
        "\n",
        "        for iteration in range(self.max_iterations):\n",
        "            print(f\"Iteration {iteration + 1}\")\n",
        "\n",
        "            # Join edges with current component mapping\n",
        "            merged_nodes = bidirectional_edges.join(current_nodes,\n",
        "                bidirectional_edges.Origin_airport == current_nodes.airport, \"left\") \\\n",
        "                .select(\n",
        "                    col(\"Destination_airport\").alias(\"airport\"),\n",
        "                    col(\"component\")\n",
        "                )\n",
        "\n",
        "            # Merge components by taking the minimum component label\n",
        "            current_nodes = merged_nodes.union(current_nodes) \\\n",
        "                .groupBy(\"airport\") \\\n",
        "                .agg(spark_min(\"component\").alias(\"component\"))\n",
        "\n",
        "            # Optional early stopping\n",
        "            current_component_count = current_nodes.select(\"component\").distinct().count()\n",
        "            if current_component_count == previous_component_count:\n",
        "                print(f\"Converged at iteration {iteration + 1}\")\n",
        "                break\n",
        "            previous_component_count = current_component_count\n",
        "\n",
        "        # STEP 3: Analyze Connected Components\n",
        "        print(\"Analyzing connected components...\")\n",
        "        components_df = current_nodes.groupBy(\"component\") \\\n",
        "            .agg(collect_list(\"airport\").alias(\"airports\"))\n",
        "\n",
        "        # Add component size using size() function\n",
        "        components_df = components_df.withColumn(\"size\", size(col(\"airports\")))\n",
        "\n",
        "        # Sort components by size in descending order\n",
        "        window_spec = Window.orderBy(col(\"size\").desc())\n",
        "        components_df = components_df.withColumn(\"rank\", row_number().over(window_spec))\n",
        "\n",
        "        # Collect results\n",
        "        results_rows = components_df.collect()\n",
        "\n",
        "        # Calculate performance metrics\n",
        "        end_time = time.time()\n",
        "        total_processing_time = end_time - start_time\n",
        "\n",
        "        # Prepare final results\n",
        "        results = {\n",
        "            \"number_of_components\": len(results_rows),\n",
        "            \"component_sizes\": [row[\"size\"] for row in results_rows],\n",
        "            \"largest_component_size\": results_rows[0][\"size\"] if results_rows else 0,\n",
        "            \"largest_component_airports\": results_rows[0][\"airports\"] if results_rows else [],\n",
        "            \"performance_metrics\": {\n",
        "                \"total_flights\": total_flights,\n",
        "                \"unique_airports\": unique_airports,\n",
        "                \"processing_time_seconds\": total_processing_time,\n",
        "                \"iterations_completed\": iteration + 1\n",
        "            }\n",
        "        }\n",
        "\n",
        "        print(\"Connected Components Analysis Completed!\")\n",
        "        return results\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"\n",
        "        Stops the SparkSession to free up resources.\n",
        "        \"\"\"\n",
        "        if self.spark:\n",
        "            self.spark.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Select a range of dates and print the result with performance metrics**"
      ],
      "metadata": {
        "id": "59jNnfDuAmkd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXQqr0C2qRrL",
        "outputId": "86eb2c51-6eb7-4412-ee59-2e2ac062334e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing SparkSession with optimized configurations...\n",
            "Analyzing flight network from 1990-01-01 to 1990-12-31\n",
            "Maximum iterations set to: 10\n",
            "Loading and filtering flight network data...\n",
            "Total flights in the period: 143082\n",
            "Unique airports: 238\n",
            "Computing connected components...\n",
            "Iteration 1\n",
            "Iteration 2\n",
            "Iteration 3\n",
            "Iteration 4\n",
            "Iteration 5\n",
            "Converged at iteration 5\n",
            "Analyzing connected components...\n",
            "Connected Components Analysis Completed!\n",
            "{\n",
            "  \"number_of_components\": 1,\n",
            "  \"component_sizes\": [\n",
            "    245\n",
            "  ],\n",
            "  \"largest_component_size\": 245,\n",
            "  \"largest_component_airports\": [\n",
            "    \"ABE\",\n",
            "    \"ABI\",\n",
            "    \"ABQ\",\n",
            "    \"ACT\",\n",
            "    \"ACV\",\n",
            "    \"ACY\",\n",
            "    \"ADM\",\n",
            "    \"ADQ\",\n",
            "    \"AGS\",\n",
            "    \"AIY\",\n",
            "    \"ALB\",\n",
            "    \"ALW\",\n",
            "    \"AMA\",\n",
            "    \"ANC\",\n",
            "    \"ATL\",\n",
            "    \"ATW\",\n",
            "    \"AUS\",\n",
            "    \"AVL\",\n",
            "    \"AVP\",\n",
            "    \"AZO\",\n",
            "    \"BDL\",\n",
            "    \"BFF\",\n",
            "    \"BFI\",\n",
            "    \"BFL\",\n",
            "    \"BGM\",\n",
            "    \"BGR\",\n",
            "    \"BHM\",\n",
            "    \"BIF\",\n",
            "    \"BIL\",\n",
            "    \"BIS\",\n",
            "    \"BLI\",\n",
            "    \"BNA\",\n",
            "    \"BOS\",\n",
            "    \"BRO\",\n",
            "    \"BTM\",\n",
            "    \"BTR\",\n",
            "    \"BTV\",\n",
            "    \"BUF\",\n",
            "    \"BWI\",\n",
            "    \"BZN\",\n",
            "    \"CAE\",\n",
            "    \"CAK\",\n",
            "    \"CEC\",\n",
            "    \"CHA\",\n",
            "    \"CHO\",\n",
            "    \"CHS\",\n",
            "    \"CIC\",\n",
            "    \"CID\",\n",
            "    \"CLE\",\n",
            "    \"CLM\",\n",
            "    \"CLT\",\n",
            "    \"CMH\",\n",
            "    \"CMI\",\n",
            "    \"COS\",\n",
            "    \"COU\",\n",
            "    \"CPR\",\n",
            "    \"CRP\",\n",
            "    \"CRW\",\n",
            "    \"CSG\",\n",
            "    \"CVG\",\n",
            "    \"CWA\",\n",
            "    \"CYS\",\n",
            "    \"DAL\",\n",
            "    \"DAY\",\n",
            "    \"DCA\",\n",
            "    \"DET\",\n",
            "    \"DFW\",\n",
            "    \"DLH\",\n",
            "    \"DRO\",\n",
            "    \"DSM\",\n",
            "    \"DTW\",\n",
            "    \"EAT\",\n",
            "    \"EDF\",\n",
            "    \"EFD\",\n",
            "    \"EKO\",\n",
            "    \"ELM\",\n",
            "    \"ELP\",\n",
            "    \"ERI\",\n",
            "    \"EUG\",\n",
            "    \"EVV\",\n",
            "    \"EWN\",\n",
            "    \"EWR\",\n",
            "    \"EYW\",\n",
            "    \"FAI\",\n",
            "    \"FAR\",\n",
            "    \"FAT\",\n",
            "    \"FAY\",\n",
            "    \"FCA\",\n",
            "    \"FFO\",\n",
            "    \"FLG\",\n",
            "    \"FLL\",\n",
            "    \"FMN\",\n",
            "    \"FNT\",\n",
            "    \"FOE\",\n",
            "    \"FSD\",\n",
            "    \"FTW\",\n",
            "    \"FWA\",\n",
            "    \"GCC\",\n",
            "    \"GEG\",\n",
            "    \"GFK\",\n",
            "    \"GJT\",\n",
            "    \"GNV\",\n",
            "    \"GPT\",\n",
            "    \"GRB\",\n",
            "    \"GRR\",\n",
            "    \"GSO\",\n",
            "    \"GSP\",\n",
            "    \"GTF\",\n",
            "    \"HLN\",\n",
            "    \"HNL\",\n",
            "    \"HOU\",\n",
            "    \"HSV\",\n",
            "    \"HUF\",\n",
            "    \"IAD\",\n",
            "    \"IAH\",\n",
            "    \"ICT\",\n",
            "    \"IDA\",\n",
            "    \"ILM\",\n",
            "    \"IND\",\n",
            "    \"ISO\",\n",
            "    \"ITH\",\n",
            "    \"ITO\",\n",
            "    \"JAC\",\n",
            "    \"JAN\",\n",
            "    \"JAX\",\n",
            "    \"JFK\",\n",
            "    \"JNU\",\n",
            "    \"KTN\",\n",
            "    \"LAN\",\n",
            "    \"LAS\",\n",
            "    \"LAX\",\n",
            "    \"LBB\",\n",
            "    \"LCK\",\n",
            "    \"LEX\",\n",
            "    \"LFT\",\n",
            "    \"LGA\",\n",
            "    \"LIT\",\n",
            "    \"LMT\",\n",
            "    \"LNK\",\n",
            "    \"LSE\",\n",
            "    \"LSV\",\n",
            "    \"LWS\",\n",
            "    \"LYH\",\n",
            "    \"MAF\",\n",
            "    \"MBS\",\n",
            "    \"MCE\",\n",
            "    \"MCI\",\n",
            "    \"MCN\",\n",
            "    \"MCO\",\n",
            "    \"MDT\",\n",
            "    \"MDW\",\n",
            "    \"MEM\",\n",
            "    \"MFR\",\n",
            "    \"MGM\",\n",
            "    \"MHT\",\n",
            "    \"MIA\",\n",
            "    \"MIQ\",\n",
            "    \"MKE\",\n",
            "    \"MLU\",\n",
            "    \"MOB\",\n",
            "    \"MOD\",\n",
            "    \"MOT\",\n",
            "    \"MSN\",\n",
            "    \"MSO\",\n",
            "    \"MSP\",\n",
            "    \"MSY\",\n",
            "    \"MTJ\",\n",
            "    \"MWH\",\n",
            "    \"MYR\",\n",
            "    \"NGP\",\n",
            "    \"NZC\",\n",
            "    \"OAJ\",\n",
            "    \"OAK\",\n",
            "    \"OGD\",\n",
            "    \"OGG\",\n",
            "    \"OKC\",\n",
            "    \"OMA\",\n",
            "    \"ORD\",\n",
            "    \"ORH\",\n",
            "    \"OSH\",\n",
            "    \"OXR\",\n",
            "    \"PBI\",\n",
            "    \"PDT\",\n",
            "    \"PDX\",\n",
            "    \"PGV\",\n",
            "    \"PHL\",\n",
            "    \"PHX\",\n",
            "    \"PIA\",\n",
            "    \"PIH\",\n",
            "    \"PIT\",\n",
            "    \"PNS\",\n",
            "    \"PUB\",\n",
            "    \"PUW\",\n",
            "    \"PVD\",\n",
            "    \"PWM\",\n",
            "    \"RAP\",\n",
            "    \"RDD\",\n",
            "    \"RDG\",\n",
            "    \"RDM\",\n",
            "    \"RDU\",\n",
            "    \"RFD\",\n",
            "    \"RIC\",\n",
            "    \"RNO\",\n",
            "    \"ROA\",\n",
            "    \"ROC\",\n",
            "    \"ROW\",\n",
            "    \"RST\",\n",
            "    \"SAT\",\n",
            "    \"SAV\",\n",
            "    \"SBA\",\n",
            "    \"SBN\",\n",
            "    \"SBP\",\n",
            "    \"SCK\",\n",
            "    \"SEA\",\n",
            "    \"SFO\",\n",
            "    \"SGF\",\n",
            "    \"SHD\",\n",
            "    \"SHR\",\n",
            "    \"SHV\",\n",
            "    \"SJC\",\n",
            "    \"SKA\",\n",
            "    \"SLC\",\n",
            "    \"SLE\",\n",
            "    \"SMF\",\n",
            "    \"SNA\",\n",
            "    \"SPI\",\n",
            "    \"SPS\",\n",
            "    \"STL\",\n",
            "    \"STS\",\n",
            "    \"SUX\",\n",
            "    \"SYR\",\n",
            "    \"TIK\",\n",
            "    \"TLH\",\n",
            "    \"TOL\",\n",
            "    \"TPA\",\n",
            "    \"TUL\",\n",
            "    \"TUS\",\n",
            "    \"TVC\",\n",
            "    \"TWF\",\n",
            "    \"TYS\",\n",
            "    \"UCA\",\n",
            "    \"YIP\",\n",
            "    \"YKM\",\n",
            "    \"YNG\",\n",
            "    \"YUM\"\n",
            "  ],\n",
            "  \"performance_metrics\": {\n",
            "    \"total_flights\": 143082,\n",
            "    \"unique_airports\": 238,\n",
            "    \"processing_time_seconds\": 287.1772463321686,\n",
            "    \"iterations_completed\": 5\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "connector = ImprovedConnectedComponents(\n",
        "    max_iterations=10\n",
        ")\n",
        "\n",
        "# Expand date range\n",
        "results = connector.find_connected_components(\n",
        "    flight_network=df,\n",
        "    start_date=\"1990-01-01\",\n",
        "    end_date=\"1990-12-31\"\n",
        ")\n",
        "\n",
        "# Print detailed results\n",
        "print(json.dumps(results, indent=2))\n",
        "\n",
        "# Cleanup\n",
        "connector.cleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFfmm588qRrL"
      },
      "source": [
        "# **Find out Connected Components Using Graph Frames package**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_spark_session():\n",
        "    \"\"\"\n",
        "    Initialize Spark session with GraphFrames JAR\n",
        "    \"\"\"\n",
        "    # Stop any existing SparkContext\n",
        "    if SparkContext._active_spark_context:\n",
        "        SparkContext._active_spark_context.stop()\n",
        "\n",
        "    # Configure SparkSession with GraphFrames package\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"GraphFramesExample\") \\\n",
        "        .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.0-s_2.12\") \\\n",
        "        .config(\"spark.driver.memory\", \"15g\") \\\n",
        "        .config(\"spark.executor.memory\", \"15g\") \\\n",
        "        .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    return spark"
      ],
      "metadata": {
        "id": "s5PORwxpEUDk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_flight_network(df, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Analyze flight network using GraphFrames with optimizations for large datasets\n",
        "    \"\"\"\n",
        "    # Ensure Spark session is properly configured\n",
        "    if SparkSession._instantiatedSession is None or SparkContext._active_spark_context is None:\n",
        "        print(\"Reinitializing Spark Session...\")\n",
        "        spark = setup_spark_session()\n",
        "    else:\n",
        "        spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "    try:\n",
        "        # Cache the filtered dataframe\n",
        "        filtered_flights = df.filter(\n",
        "            (F.col(\"Fly_date\") >= start_date) &\n",
        "            (F.col(\"Fly_date\") <= end_date)\n",
        "        ).cache()\n",
        "\n",
        "        print(f\"Number of flights in period: {filtered_flights.count()}\")\n",
        "\n",
        "        # Create and cache vertices\n",
        "        vertices = filtered_flights.select(\n",
        "            F.col(\"Origin_airport\").alias(\"id\")\n",
        "        ).union(\n",
        "            filtered_flights.select(\n",
        "                F.col(\"Destination_airport\").alias(\"id\")\n",
        "            )\n",
        "        ).distinct().cache()\n",
        "\n",
        "        print(f\"Number of unique airports: {vertices.count()}\")\n",
        "\n",
        "        # Create and cache edges\n",
        "        edges = filtered_flights.select(\n",
        "            F.col(\"Origin_airport\").alias(\"src\"),\n",
        "            F.col(\"Destination_airport\").alias(\"dst\")\n",
        "        ).distinct().cache()\n",
        "\n",
        "        print(f\"Number of unique routes: {edges.count()}\")\n",
        "\n",
        "        # Create graph\n",
        "        print(\"Creating GraphFrame...\")\n",
        "        graph = GraphFrame(vertices, edges)\n",
        "\n",
        "        # Run connected components with checkpoint\n",
        "        print(\"Running connected components analysis...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Use strongly connected components instead of connected components\n",
        "        # This is often more memory-efficient\n",
        "        result = graph.stronglyConnectedComponents(maxIter=10)\n",
        "\n",
        "        execution_time = time.time() - start_time\n",
        "        print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "\n",
        "        # Analyze results in chunks\n",
        "        print(\"\\nAnalyzing components...\")\n",
        "        num_components = result.select(\"component\").distinct().count()\n",
        "        print(f\"Number of connected components: {num_components}\")\n",
        "\n",
        "        # Get component sizes efficiently\n",
        "        print(\"\\nComponent sizes (top 10 largest):\")\n",
        "        component_sizes = result.groupBy(\"component\") \\\n",
        "            .count() \\\n",
        "            .orderBy(F.desc(\"count\")) \\\n",
        "            .limit(10)\n",
        "        component_sizes.show()\n",
        "\n",
        "        # Get largest component efficiently\n",
        "        largest_comp_id = component_sizes.first()[\"component\"]\n",
        "        largest_comp_size = component_sizes.first()[\"count\"]\n",
        "\n",
        "        print(f\"\\nLargest component has {largest_comp_size} airports\")\n",
        "        print(\"Sample of airports in largest component (up to 10):\")\n",
        "        result.filter(F.col(\"component\") == largest_comp_id) \\\n",
        "            .select(\"id\") \\\n",
        "            .limit(10) \\\n",
        "            .show()\n",
        "\n",
        "        # Clean up\n",
        "        filtered_flights.unpersist()\n",
        "        vertices.unpersist()\n",
        "        edges.unpersist()\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    finally:\n",
        "        # Clean up checkpoint directory\n",
        "        checkpoint_dir = spark.sparkContext.getCheckpointDir()\n",
        "        if checkpoint_dir and os.path.exists(checkpoint_dir):\n",
        "            try:\n",
        "                import shutil\n",
        "                shutil.rmtree(checkpoint_dir)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "def print_basic_stats(df, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Print basic statistics about the dataset before running full analysis\n",
        "    \"\"\"\n",
        "    # Reinitialize Spark session if not active\n",
        "    if SparkSession._instantiatedSession is None or SparkContext._active_spark_context is None:\n",
        "        print(\"Reinitializing Spark Session...\")\n",
        "        spark = SparkSession.builder \\\n",
        "            .appName(\"ConnectedComponentsGraphFrames\") \\\n",
        "            .config(\"spark.driver.memory\", \"15g\") \\\n",
        "            .getOrCreate()\n",
        "\n",
        "    print(\"Dataset Statistics:\")\n",
        "    df.filter(\n",
        "        (F.col(\"Fly_date\") >= start_date) &\n",
        "        (F.col(\"Fly_date\") <= end_date)\n",
        "    ).agg(\n",
        "        F.countDistinct(\"Origin_airport\").alias(\"unique_origins\"),\n",
        "        F.countDistinct(\"Destination_airport\").alias(\"unique_destinations\"),\n",
        "        F.count(\"*\").alias(\"total_flights\")\n",
        "    ).show()\n"
      ],
      "metadata": {
        "id": "XtzcHpyPweba"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = setup_spark_session()\n",
        "df = spark.read.csv(\"/Airports2.csv\", header=True, inferSchema=True)\n",
        "print_basic_stats(df, '1990-01-01', '1990-12-31')\n",
        "result = analyze_flight_network(df, '1990-01-01', '1990-12-31')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0j-nzX5wgVd",
        "outputId": "5243894b-0ed6-402d-8253-608fddd6308d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Statistics:\n",
            "+--------------+-------------------+-------------+\n",
            "|unique_origins|unique_destinations|total_flights|\n",
            "+--------------+-------------------+-------------+\n",
            "|           238|                242|       143082|\n",
            "+--------------+-------------------+-------------+\n",
            "\n",
            "Number of flights in period: 143082\n",
            "Number of unique airports: 245\n",
            "Number of unique routes: 6035\n",
            "Creating GraphFrame...\n",
            "Running connected components analysis...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
            "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time: 286.22 seconds\n",
            "\n",
            "Analyzing components...\n",
            "Number of connected components: 11\n",
            "\n",
            "Component sizes (top 10 largest):\n",
            "+------------+-----+\n",
            "|   component|count|\n",
            "+------------+-----+\n",
            "|           0|  235|\n",
            "|463856467969|    1|\n",
            "|523986010112|    1|\n",
            "|558345748480|    1|\n",
            "|146028888065|    1|\n",
            "|163208757249|    1|\n",
            "|240518168576|    1|\n",
            "|274877906947|    1|\n",
            "|781684047872|    1|\n",
            "|352187318273|    1|\n",
            "+------------+-----+\n",
            "\n",
            "\n",
            "Largest component has 235 airports\n",
            "Sample of airports in largest component (up to 10):\n",
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|HNL|\n",
            "|BGM|\n",
            "|BNA|\n",
            "|BLI|\n",
            "|ERI|\n",
            "|CLT|\n",
            "|TVC|\n",
            "|SHV|\n",
            "|CIC|\n",
            "|CVG|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}