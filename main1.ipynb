{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bonus Question - Connected Components on MapReduce** \n",
    "MapReduce is ideal for network analysis as it enables parallel processing of large graph datasets, making it scalable and efficient. By breaking tasks into map and reduce steps, it allows for distributed analysis of networks, which is essential for handling large-scale graph problems like connected components.\n",
    "\n",
    "1. In this task, you are required to use PySpark and the MapReduce paradigm to identify the connected components in a flight network graph. The focus should be on airports rather than cities. As you know, a connected component refers to a group of airports where every pair of airports within the group is connected either directly or indirectly.\n",
    "\n",
    "The function takes the following inputs: \n",
    "1. Flight network\n",
    "2. A starting date\n",
    "3. An end date\n",
    "\n",
    "The function outputs: \n",
    "1. The number of the connected components during that period\n",
    "2. The size of each connectd componenet\n",
    "3. The airports within the largest connected component identified.\n",
    "\n",
    "__Note:__ For this task, you should check if there is a flight between two airports during that period.\n",
    "__Note:__ You are not allowed to use pre-existing packages or functions in PySpark; instead, you must implement the algorithm from scratch using the MapReduce paradigm.\n",
    "\n",
    "2. Compare the execution time and the results of your implementation with those of the GraphFrames package for identifying connected components. If there is any difference in the results, provide an explanation for why that might occur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyngrok in c:\\users\\hp\\anaconda3\\lib\\site-packages (7.2.1)\n",
      "Requirement already satisfied: gdown in c:\\users\\hp\\anaconda3\\lib\\site-packages (5.2.0)\n",
      "Requirement already satisfied: pyspark in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: yellowbrick in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.5)\n",
      "Collecting graphframes\n",
      "  Downloading graphframes-0.6-py2.py3-none-any.whl.metadata (934 bytes)\n",
      "Requirement already satisfied: PyYAML>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyngrok) (6.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gdown) (3.13.1)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gdown) (2.32.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gdown) (4.66.4)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from yellowbrick) (3.8.4)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from yellowbrick) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from yellowbrick) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from yellowbrick) (1.26.4)\n",
      "Requirement already satisfied: cycler>=0.10.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from yellowbrick) (0.11.0)\n",
      "Collecting nose (from graphframes)\n",
      "  Downloading nose-1.3.7-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.9.0.post0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->yellowbrick) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->yellowbrick) (2.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.16.0)\n",
      "Downloading graphframes-0.6-py2.py3-none-any.whl (18 kB)\n",
      "Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
      "   ---------------------------------------- 0.0/154.7 kB ? eta -:--:--\n",
      "   ----------------------- ---------------- 92.2/154.7 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 154.7/154.7 kB 3.1 MB/s eta 0:00:00\n",
      "Installing collected packages: nose, graphframes\n",
      "Successfully installed graphframes-0.6 nose-1.3.7\n"
     ]
    }
   ],
   "source": [
    "! pip install pyngrok gdown  pyspark  yellowbrick graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Caricamento del file CSV\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAirposrts2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Mostra i primi record per confermare\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Caricamento del file CSV\n",
    "df = spark.read.csv(\"Airposrts2.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Mostra i primi record per confermare\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, collect_list, min as spark_min, struct, size, row_number\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "class ImprovedConnectedComponents:\n",
    "    def __init__(self, max_iterations=20, spark_configs=None):\n",
    "        \"\"\"\n",
    "        Initializes SparkSession with optimized configurations.\n",
    "\n",
    "        Args:\n",
    "            max_iterations (int, optional): Maximum number of iterations for component detection. Defaults to 20.\n",
    "            spark_configs (dict, optional): Additional Spark configurations as key-value pairs.\n",
    "        \"\"\"\n",
    "        print(\"Initializing SparkSession with optimized configurations...\")\n",
    "        \n",
    "        builder = SparkSession.builder.appName(\"Optimized Connected Components\")\n",
    "        default_configs = {\n",
    "            \"spark.sql.adaptive.enabled\": \"true\",\n",
    "            \"spark.sql.adaptive.skewJoin.enabled\": \"true\",\n",
    "            \"spark.sql.shuffle.partitions\": \"400\",\n",
    "            \"spark.driver.memory\": \"16g\",\n",
    "            \"spark.executor.memory\": \"16g\",\n",
    "            \"spark.memory.fraction\": \"0.8\",\n",
    "            \"spark.memory.storageFraction\": \"0.2\"\n",
    "        }\n",
    "        # Merge default and custom configurations\n",
    "        for key, value in {**default_configs, **(spark_configs or {})}.items():\n",
    "            builder = builder.config(key, value)\n",
    "        \n",
    "        self.spark = builder.getOrCreate()\n",
    "        self.sc = self.spark.sparkContext\n",
    "        self.max_iterations = max_iterations\n",
    "\n",
    "    def find_connected_components(self, flight_network_path, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Identifies connected components in a flight network using an optimized algorithm.\n",
    "\n",
    "        Args:\n",
    "            flight_network_path (str): Path to the flight network CSV file.\n",
    "            start_date (str): Start date for filtering (format YYYY-MM-DD).\n",
    "            end_date (str): End date for filtering (format YYYY-MM-DD).\n",
    "\n",
    "        Returns:\n",
    "            dict: Results containing connected components information and performance metrics.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        print(f\"Analyzing flight network from {start_date} to {end_date}\")\n",
    "        print(f\"Maximum iterations set to: {self.max_iterations}\")\n",
    "\n",
    "        # STEP 1: Load and filter the dataset\n",
    "        print(\"Loading and filtering flight network data...\")\n",
    "        df = self.spark.read.csv(flight_network_path, header=True, inferSchema=True)\n",
    "        \n",
    "        # Check if necessary columns exist\n",
    "        required_columns = ['Fly_date', 'Origin_airport', 'Destination_airport']\n",
    "        for col_name in required_columns:\n",
    "            if col_name not in df.columns:\n",
    "                raise ValueError(f\"Required column '{col_name}' not found in the dataset\")\n",
    "        \n",
    "        # Filter by date range\n",
    "        filtered_df = df.filter((col(\"Fly_date\") >= start_date) & (col(\"Fly_date\") <= end_date))\n",
    "        \n",
    "        # Print dataset statistics\n",
    "        total_flights = filtered_df.count()\n",
    "        unique_airports = filtered_df.select(\"Origin_airport\").distinct().count()\n",
    "        print(f\"Total flights in the period: {total_flights}\")\n",
    "        print(f\"Unique airports: {unique_airports}\")\n",
    "\n",
    "        # Create undirected graph edges\n",
    "        edges_df = filtered_df.select(\"Origin_airport\", \"Destination_airport\").distinct()\n",
    "        bidirectional_edges = edges_df.union(\n",
    "            edges_df.select(\n",
    "                col(\"Destination_airport\").alias(\"Origin_airport\"),\n",
    "                col(\"Origin_airport\").alias(\"Destination_airport\")\n",
    "            )\n",
    "        ).distinct()\n",
    "\n",
    "        # STEP 2: Connected Components using Disjoint Set (Union-Find) algorithm\n",
    "        print(\"Computing connected components...\")\n",
    "        \n",
    "        # Initial node mapping\n",
    "        initial_nodes = bidirectional_edges.select(\"Origin_airport\").distinct() \\\n",
    "            .withColumnRenamed(\"Origin_airport\", \"airport\") \\\n",
    "            .withColumn(\"component\", col(\"airport\"))\n",
    "\n",
    "        # Iterative component merging\n",
    "        current_nodes = initial_nodes\n",
    "        previous_component_count = 0\n",
    "\n",
    "        for iteration in range(self.max_iterations):\n",
    "            print(f\"Iteration {iteration + 1}\")\n",
    "            \n",
    "            # Join edges with current component mapping\n",
    "            merged_nodes = bidirectional_edges.join(current_nodes, \n",
    "                bidirectional_edges.Origin_airport == current_nodes.airport, \"left\") \\\n",
    "                .select(\n",
    "                    col(\"Destination_airport\").alias(\"airport\"),\n",
    "                    col(\"component\")\n",
    "                )\n",
    "\n",
    "            # Merge components by taking the minimum component label\n",
    "            current_nodes = merged_nodes.union(current_nodes) \\\n",
    "                .groupBy(\"airport\") \\\n",
    "                .agg(spark_min(\"component\").alias(\"component\"))\n",
    "\n",
    "            # Optional early stopping\n",
    "            current_component_count = current_nodes.select(\"component\").distinct().count()\n",
    "            if current_component_count == previous_component_count:\n",
    "                print(f\"Converged at iteration {iteration + 1}\")\n",
    "                break\n",
    "            previous_component_count = current_component_count\n",
    "\n",
    "        # STEP 3: Analyze Connected Components\n",
    "        print(\"Analyzing connected components...\")\n",
    "        components_df = current_nodes.groupBy(\"component\") \\\n",
    "            .agg(collect_list(\"airport\").alias(\"airports\"))\n",
    "        \n",
    "        # Add component size using size() function\n",
    "        components_df = components_df.withColumn(\"size\", size(col(\"airports\")))\n",
    "        \n",
    "        # Sort components by size in descending order\n",
    "        window_spec = Window.orderBy(col(\"size\").desc())\n",
    "        components_df = components_df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "        # Collect results\n",
    "        results_rows = components_df.collect()\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        end_time = time.time()\n",
    "        total_processing_time = end_time - start_time\n",
    "        \n",
    "        # Prepare final results\n",
    "        results = {\n",
    "            \"number_of_components\": len(results_rows),\n",
    "            \"component_sizes\": [row[\"size\"] for row in results_rows],\n",
    "            \"largest_component_size\": results_rows[0][\"size\"] if results_rows else 0,\n",
    "            \"largest_component_airports\": results_rows[0][\"airports\"] if results_rows else [],\n",
    "            \"performance_metrics\": {\n",
    "                \"total_flights\": total_flights,\n",
    "                \"unique_airports\": unique_airports,\n",
    "                \"processing_time_seconds\": total_processing_time,\n",
    "                \"iterations_completed\": iteration + 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "        print(\"Connected Components Analysis Completed!\")\n",
    "        return results\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"\n",
    "        Stops the SparkSession to free up resources.\n",
    "        \"\"\"\n",
    "        if self.spark:\n",
    "            self.spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing SparkSession with optimized configurations...\n",
      "Analyzing flight network from 1990-01-01 to 2000-12-31\n",
      "Maximum iterations set to: 10\n",
      "Loading and filtering flight network data...\n",
      "Total flights in the period: 1650268\n",
      "Unique airports: 450\n",
      "Computing connected components...\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Converged at iteration 5\n",
      "Analyzing connected components...\n",
      "Connected Components Analysis Completed!\n",
      "{\n",
      "  \"number_of_components\": 1,\n",
      "  \"component_sizes\": [\n",
      "    463\n",
      "  ],\n",
      "  \"largest_component_size\": 463,\n",
      "  \"largest_component_airports\": [\n",
      "    \"ABE\",\n",
      "    \"ABI\",\n",
      "    \"ABQ\",\n",
      "    \"ABR\",\n",
      "    \"ABY\",\n",
      "    \"ACT\",\n",
      "    \"ACV\",\n",
      "    \"ACY\",\n",
      "    \"ADM\",\n",
      "    \"ADQ\",\n",
      "    \"ADS\",\n",
      "    \"AEX\",\n",
      "    \"AFW\",\n",
      "    \"AGC\",\n",
      "    \"AGS\",\n",
      "    \"AHN\",\n",
      "    \"AID\",\n",
      "    \"AIY\",\n",
      "    \"ALB\",\n",
      "    \"ALM\",\n",
      "    \"ALO\",\n",
      "    \"ALW\",\n",
      "    \"AMA\",\n",
      "    \"ANB\",\n",
      "    \"ANC\",\n",
      "    \"AND\",\n",
      "    \"AOH\",\n",
      "    \"AOO\",\n",
      "    \"APF\",\n",
      "    \"APN\",\n",
      "    \"ARA\",\n",
      "    \"ART\",\n",
      "    \"AST\",\n",
      "    \"ATL\",\n",
      "    \"ATW\",\n",
      "    \"ATY\",\n",
      "    \"AUS\",\n",
      "    \"AVL\",\n",
      "    \"AVP\",\n",
      "    \"AXV\",\n",
      "    \"AZO\",\n",
      "    \"BAD\",\n",
      "    \"BDL\",\n",
      "    \"BDR\",\n",
      "    \"BFD\",\n",
      "    \"BFF\",\n",
      "    \"BFI\",\n",
      "    \"BFL\",\n",
      "    \"BFM\",\n",
      "    \"BFR\",\n",
      "    \"BGM\",\n",
      "    \"BGR\",\n",
      "    \"BGS\",\n",
      "    \"BHM\",\n",
      "    \"BIF\",\n",
      "    \"BIL\",\n",
      "    \"BIS\",\n",
      "    \"BJI\",\n",
      "    \"BJJ\",\n",
      "    \"BKW\",\n",
      "    \"BLI\",\n",
      "    \"BMC\",\n",
      "    \"BMG\",\n",
      "    \"BMI\",\n",
      "    \"BNA\",\n",
      "    \"BOS\",\n",
      "    \"BPT\",\n",
      "    \"BQK\",\n",
      "    \"BRD\",\n",
      "    \"BRL\",\n",
      "    \"BRO\",\n",
      "    \"BSM\",\n",
      "    \"BTL\",\n",
      "    \"BTM\",\n",
      "    \"BTR\",\n",
      "    \"BTV\",\n",
      "    \"BUF\",\n",
      "    \"BVX\",\n",
      "    \"BWG\",\n",
      "    \"BWI\",\n",
      "    \"BYH\",\n",
      "    \"BZN\",\n",
      "    \"CAE\",\n",
      "    \"CAK\",\n",
      "    \"CBE\",\n",
      "    \"CDC\",\n",
      "    \"CEC\",\n",
      "    \"CEV\",\n",
      "    \"CGI\",\n",
      "    \"CGX\",\n",
      "    \"CHA\",\n",
      "    \"CHO\",\n",
      "    \"CHS\",\n",
      "    \"CIC\",\n",
      "    \"CID\",\n",
      "    \"CKB\",\n",
      "    \"CLE\",\n",
      "    \"CLL\",\n",
      "    \"CLM\",\n",
      "    \"CLT\",\n",
      "    \"CMH\",\n",
      "    \"CMI\",\n",
      "    \"CNM\",\n",
      "    \"CNW\",\n",
      "    \"COS\",\n",
      "    \"COU\",\n",
      "    \"CPR\",\n",
      "    \"CRP\",\n",
      "    \"CRS\",\n",
      "    \"CRW\",\n",
      "    \"CSG\",\n",
      "    \"CSV\",\n",
      "    \"CVG\",\n",
      "    \"CVN\",\n",
      "    \"CVS\",\n",
      "    \"CWA\",\n",
      "    \"CYS\",\n",
      "    \"DAL\",\n",
      "    \"DAY\",\n",
      "    \"DBQ\",\n",
      "    \"DCA\",\n",
      "    \"DCU\",\n",
      "    \"DDC\",\n",
      "    \"DEC\",\n",
      "    \"DET\",\n",
      "    \"DFW\",\n",
      "    \"DHN\",\n",
      "    \"DLH\",\n",
      "    \"DNV\",\n",
      "    \"DOV\",\n",
      "    \"DPA\",\n",
      "    \"DRO\",\n",
      "    \"DRT\",\n",
      "    \"DSM\",\n",
      "    \"DTW\",\n",
      "    \"EAT\",\n",
      "    \"EAU\",\n",
      "    \"EDF\",\n",
      "    \"EFD\",\n",
      "    \"EGP\",\n",
      "    \"EIL\",\n",
      "    \"EKO\",\n",
      "    \"ELD\",\n",
      "    \"ELM\",\n",
      "    \"ELP\",\n",
      "    \"ERI\",\n",
      "    \"ESC\",\n",
      "    \"ESF\",\n",
      "    \"EUG\",\n",
      "    \"EVV\",\n",
      "    \"EWN\",\n",
      "    \"EWR\",\n",
      "    \"EYW\",\n",
      "    \"FAI\",\n",
      "    \"FAM\",\n",
      "    \"FAR\",\n",
      "    \"FAT\",\n",
      "    \"FAY\",\n",
      "    \"FBK\",\n",
      "    \"FCA\",\n",
      "    \"FDY\",\n",
      "    \"FFO\",\n",
      "    \"FLG\",\n",
      "    \"FLL\",\n",
      "    \"FLO\",\n",
      "    \"FMN\",\n",
      "    \"FNT\",\n",
      "    \"FOD\",\n",
      "    \"FOE\",\n",
      "    \"FSD\",\n",
      "    \"FSM\",\n",
      "    \"FTW\",\n",
      "    \"FWA\",\n",
      "    \"FWH\",\n",
      "    \"FYV\",\n",
      "    \"GBD\",\n",
      "    \"GCC\",\n",
      "    \"GCK\",\n",
      "    \"GCY\",\n",
      "    \"GDC\",\n",
      "    \"GEG\",\n",
      "    \"GFK\",\n",
      "    \"GGG\",\n",
      "    \"GJT\",\n",
      "    \"GLH\",\n",
      "    \"GNV\",\n",
      "    \"GPT\",\n",
      "    \"GRB\",\n",
      "    \"GRI\",\n",
      "    \"GRK\",\n",
      "    \"GRR\",\n",
      "    \"GSB\",\n",
      "    \"GSO\",\n",
      "    \"GSP\",\n",
      "    \"GTF\",\n",
      "    \"GTR\",\n",
      "    \"GUP\",\n",
      "    \"GWO\",\n",
      "    \"GYY\",\n",
      "    \"HGR\",\n",
      "    \"HIF\",\n",
      "    \"HII\",\n",
      "    \"HIK\",\n",
      "    \"HKY\",\n",
      "    \"HLN\",\n",
      "    \"HNL\",\n",
      "    \"HOB\",\n",
      "    \"HOU\",\n",
      "    \"HRO\",\n",
      "    \"HSV\",\n",
      "    \"HUF\",\n",
      "    \"HUM\",\n",
      "    \"HVN\",\n",
      "    \"HYS\",\n",
      "    \"IAB\",\n",
      "    \"IAD\",\n",
      "    \"IAH\",\n",
      "    \"ICT\",\n",
      "    \"IDA\",\n",
      "    \"ILE\",\n",
      "    \"ILG\",\n",
      "    \"ILM\",\n",
      "    \"ILN\",\n",
      "    \"IMT\",\n",
      "    \"IND\",\n",
      "    \"IPL\",\n",
      "    \"IPT\",\n",
      "    \"ISO\",\n",
      "    \"ITH\",\n",
      "    \"ITO\",\n",
      "    \"JAC\",\n",
      "    \"JAN\",\n",
      "    \"JAX\",\n",
      "    \"JBR\",\n",
      "    \"JFK\",\n",
      "    \"JHW\",\n",
      "    \"JLN\",\n",
      "    \"JNU\",\n",
      "    \"JXN\",\n",
      "    \"KTN\",\n",
      "    \"LAF\",\n",
      "    \"LAM\",\n",
      "    \"LAN\",\n",
      "    \"LAR\",\n",
      "    \"LAS\",\n",
      "    \"LAW\",\n",
      "    \"LAX\",\n",
      "    \"LBB\",\n",
      "    \"LBF\",\n",
      "    \"LBL\",\n",
      "    \"LCH\",\n",
      "    \"LCK\",\n",
      "    \"LEX\",\n",
      "    \"LFK\",\n",
      "    \"LFT\",\n",
      "    \"LGA\",\n",
      "    \"LIT\",\n",
      "    \"LMT\",\n",
      "    \"LNK\",\n",
      "    \"LNS\",\n",
      "    \"LRD\",\n",
      "    \"LRU\",\n",
      "    \"LSE\",\n",
      "    \"LSF\",\n",
      "    \"LSV\",\n",
      "    \"LTS\",\n",
      "    \"LUK\",\n",
      "    \"LWS\",\n",
      "    \"LYH\",\n",
      "    \"MAF\",\n",
      "    \"MBS\",\n",
      "    \"MCE\",\n",
      "    \"MCI\",\n",
      "    \"MCN\",\n",
      "    \"MCO\",\n",
      "    \"MCW\",\n",
      "    \"MDH\",\n",
      "    \"MDT\",\n",
      "    \"MDW\",\n",
      "    \"MEI\",\n",
      "    \"MEM\",\n",
      "    \"MFD\",\n",
      "    \"MFR\",\n",
      "    \"MGM\",\n",
      "    \"MGW\",\n",
      "    \"MGY\",\n",
      "    \"MHK\",\n",
      "    \"MHL\",\n",
      "    \"MHR\",\n",
      "    \"MHT\",\n",
      "    \"MIA\",\n",
      "    \"MIB\",\n",
      "    \"MIE\",\n",
      "    \"MIQ\",\n",
      "    \"MKC\",\n",
      "    \"MKE\",\n",
      "    \"MKG\",\n",
      "    \"MKL\",\n",
      "    \"MLU\",\n",
      "    \"MMI\",\n",
      "    \"MML\",\n",
      "    \"MOB\",\n",
      "    \"MOD\",\n",
      "    \"MOP\",\n",
      "    \"MOR\",\n",
      "    \"MOT\",\n",
      "    \"MQT\",\n",
      "    \"MRC\",\n",
      "    \"MSN\",\n",
      "    \"MSO\",\n",
      "    \"MSP\",\n",
      "    \"MSY\",\n",
      "    \"MTJ\",\n",
      "    \"MUO\",\n",
      "    \"MUT\",\n",
      "    \"MVN\",\n",
      "    \"MWA\",\n",
      "    \"MWH\",\n",
      "    \"MYR\",\n",
      "    \"MZZ\",\n",
      "    \"NEW\",\n",
      "    \"NFL\",\n",
      "    \"NGP\",\n",
      "    \"NKX\",\n",
      "    \"NQA\",\n",
      "    \"NQX\",\n",
      "    \"NZC\",\n",
      "    \"NZJ\",\n",
      "    \"NZY\",\n",
      "    \"OAJ\",\n",
      "    \"OAK\",\n",
      "    \"OGD\",\n",
      "    \"OGG\",\n",
      "    \"OGS\",\n",
      "    \"OKC\",\n",
      "    \"OKK\",\n",
      "    \"OLM\",\n",
      "    \"OLS\",\n",
      "    \"OMA\",\n",
      "    \"OPF\",\n",
      "    \"ORD\",\n",
      "    \"ORH\",\n",
      "    \"OSH\",\n",
      "    \"OWB\",\n",
      "    \"OXR\",\n",
      "    \"PAH\",\n",
      "    \"PBI\",\n",
      "    \"PDK\",\n",
      "    \"PDT\",\n",
      "    \"PDX\",\n",
      "    \"PFN\",\n",
      "    \"PGV\",\n",
      "    \"PHL\",\n",
      "    \"PHX\",\n",
      "    \"PIA\",\n",
      "    \"PIB\",\n",
      "    \"PIH\",\n",
      "    \"PIR\",\n",
      "    \"PIT\",\n",
      "    \"PKB\",\n",
      "    \"PNS\",\n",
      "    \"POB\",\n",
      "    \"PRB\",\n",
      "    \"PRC\",\n",
      "    \"PUB\",\n",
      "    \"PUW\",\n",
      "    \"PVD\",\n",
      "    \"PWK\",\n",
      "    \"PWM\",\n",
      "    \"RAP\",\n",
      "    \"RCA\",\n",
      "    \"RDD\",\n",
      "    \"RDG\",\n",
      "    \"RDM\",\n",
      "    \"RDU\",\n",
      "    \"RFD\",\n",
      "    \"RIC\",\n",
      "    \"RIV\",\n",
      "    \"RIW\",\n",
      "    \"RKS\",\n",
      "    \"RNO\",\n",
      "    \"ROA\",\n",
      "    \"ROC\",\n",
      "    \"ROW\",\n",
      "    \"RST\",\n",
      "    \"RWI\",\n",
      "    \"SAF\",\n",
      "    \"SAN\",\n",
      "    \"SAT\",\n",
      "    \"SAV\",\n",
      "    \"SAW\",\n",
      "    \"SBA\",\n",
      "    \"SBM\",\n",
      "    \"SBN\",\n",
      "    \"SBP\",\n",
      "    \"SCF\",\n",
      "    \"SCK\",\n",
      "    \"SDM\",\n",
      "    \"SEA\",\n",
      "    \"SEM\",\n",
      "    \"SFB\",\n",
      "    \"SFO\",\n",
      "    \"SGF\",\n",
      "    \"SGH\",\n",
      "    \"SHD\",\n",
      "    \"SHN\",\n",
      "    \"SHR\",\n",
      "    \"SHV\",\n",
      "    \"SIK\",\n",
      "    \"SJC\",\n",
      "    \"SJT\",\n",
      "    \"SKA\",\n",
      "    \"SKF\",\n",
      "    \"SLC\",\n",
      "    \"SLE\",\n",
      "    \"SLN\",\n",
      "    \"SMF\",\n",
      "    \"SNA\",\n",
      "    \"SPI\",\n",
      "    \"SPS\",\n",
      "    \"SSI\",\n",
      "    \"STC\",\n",
      "    \"STJ\",\n",
      "    \"STL\",\n",
      "    \"STP\",\n",
      "    \"STS\",\n",
      "    \"SUS\",\n",
      "    \"SUX\",\n",
      "    \"SVC\",\n",
      "    \"SVN\",\n",
      "    \"SWO\",\n",
      "    \"SYI\",\n",
      "    \"SYR\",\n",
      "    \"TBN\",\n",
      "    \"TCL\",\n",
      "    \"TCM\",\n",
      "    \"TIK\",\n",
      "    \"TKF\",\n",
      "    \"TLH\",\n",
      "    \"TNT\",\n",
      "    \"TOL\",\n",
      "    \"TPA\",\n",
      "    \"TTN\",\n",
      "    \"TUL\",\n",
      "    \"TUP\",\n",
      "    \"TUS\",\n",
      "    \"TVC\",\n",
      "    \"TWF\",\n",
      "    \"TYR\",\n",
      "    \"TYS\",\n",
      "    \"UBS\",\n",
      "    \"UCA\",\n",
      "    \"UIN\",\n",
      "    \"VAD\",\n",
      "    \"VCT\",\n",
      "    \"VLD\",\n",
      "    \"WGO\",\n",
      "    \"WRB\",\n",
      "    \"XNA\",\n",
      "    \"YIP\",\n",
      "    \"YKM\",\n",
      "    \"YNG\",\n",
      "    \"YUM\"\n",
      "  ],\n",
      "  \"performance_metrics\": {\n",
      "    \"total_flights\": 1650268,\n",
      "    \"unique_airports\": 450,\n",
      "    \"processing_time_seconds\": 71.18465328216553,\n",
      "    \"iterations_completed\": 5\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Initialize with more iterations\n",
    "import json\n",
    "\n",
    "\n",
    "connector = ImprovedConnectedComponents(\n",
    "    max_iterations=10  # Increased from 10 to 50\n",
    ")\n",
    "\n",
    "# Expand date range\n",
    "results = connector.find_connected_components(\n",
    "    flight_network_path=\"Airports2.csv\",\n",
    "    start_date=\"1990-01-01\",\n",
    "    end_date=\"2000-12-31\"\n",
    ")\n",
    "\n",
    "# Print detailed results\n",
    "print(json.dumps(results, indent=2))\n",
    "\n",
    "# Cleanup\n",
    "connector.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Graph Frames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from graphframes import GraphFrame\n",
    "from pyspark.sql.functions import col, explode\n",
    "\n",
    "def analyze_flight_network_graphframes(flight_network_df, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Analyze connected components in a flight network using GraphFrames\n",
    "    \n",
    "    Parameters:\n",
    "    - flight_network_df: PySpark DataFrame with flight network data\n",
    "    - start_date: Beginning of the date range to analyze\n",
    "    - end_date: End of the date range to analyze\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    # 1. Filter flights within the specified date range\n",
    "    filtered_flights = flight_network_df.filter(\n",
    "        (col(\"date\") >= start_date) & (col(\"date\") <= end_date)\n",
    "    )\n",
    "    \n",
    "    # 2. Create vertices (unique airports)\n",
    "    vertices = filtered_flights.select(\n",
    "        col(\"origin\").alias(\"id\")\n",
    "    ).union(\n",
    "        filtered_flights.select(\n",
    "            col(\"destination\").alias(\"id\")\n",
    "        )\n",
    "    ).distinct()\n",
    "    \n",
    "    # 3. Create edges (flights between airports)\n",
    "    edges = filtered_flights.select(\n",
    "        col(\"origin\").alias(\"src\"),\n",
    "        col(\"destination\").alias(\"dst\")\n",
    "    )\n",
    "    \n",
    "    # 4. Create GraphFrame\n",
    "    graph = GraphFrame(vertices, edges)\n",
    "    \n",
    "    # 5. Find connected components\n",
    "    connected_components = graph.connectedComponents()\n",
    "    \n",
    "    # 6. Analyze components\n",
    "    component_analysis = connected_components.groupBy(\"component\") \\\n",
    "        .agg(\n",
    "            F.count(\"id\").alias(\"component_size\"),\n",
    "            F.collect_list(\"id\").alias(\"airports_in_component\")\n",
    "        ).orderBy(col(\"component_size\").desc())\n",
    "    \n",
    "    # 7. Prepare results\n",
    "    results = {\n",
    "        \"total_connected_components\": component_analysis.count(),\n",
    "        \"component_sizes\": [row[\"component_size\"] for row in component_analysis.collect()],\n",
    "        \"largest_component_airports\": component_analysis.first()[\"airports_in_component\"]\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphframes in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from graphframes) (1.26.4)\n",
      "Requirement already satisfied: nose in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from graphframes) (1.3.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install graphframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1139)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1125)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:489)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1790)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$16(SparkContext.scala:528)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$16$adapted(SparkContext.scala:528)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:528)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functions \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Initialize Spark session\u001b[39;00m\n\u001b[0;32m      7\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConnectedComponentsGraphFrames\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.jars.packages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgraphframes:graphframes:0.8.2-spark3.0-s_2.12\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m---> 10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_connected_components\u001b[39m(flight_network_df, start_date, end_date):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# 1. Filter flights within the specified date range\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     filtered_flights \u001b[38;5;241m=\u001b[39m flight_network_df\u001b[38;5;241m.\u001b[39mfilter(\n\u001b[0;32m     15\u001b[0m         (F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFly_date\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m start_date) \u001b[38;5;241m&\u001b[39m (F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFly_date\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m end_date)\n\u001b[0;32m     16\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1139)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1125)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:489)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1790)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$16(SparkContext.scala:528)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$16$adapted(SparkContext.scala:528)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:528)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "from graphframes import GraphFrame\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ConnectedComponentsGraphFrames\") \\\n",
    "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.0-s_2.12\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def find_connected_components(flight_network_df, start_date, end_date):\n",
    "    # 1. Filter flights within the specified date range\n",
    "    filtered_flights = flight_network_df.filter(\n",
    "        (F.col(\"Fly_date\") >= start_date) & (F.col(\"Fly_date\") <= end_date)\n",
    "    )\n",
    "\n",
    "    # 2. Create vertices (unique airports)\n",
    "    vertices = filtered_flights.select(\n",
    "        F.col(\"Origin_airport\").alias(\"id\")\n",
    "    ).union(\n",
    "        filtered_flights.select(\n",
    "            F.col(\"Destination_airport\").alias(\"id\")\n",
    "        )\n",
    "    ).distinct()\n",
    "\n",
    "    # 3. Create edges (flights between airports)\n",
    "    edges = filtered_flights.select(\n",
    "        F.col(\"Origin_airport\").alias(\"src\"),\n",
    "        F.col(\"Destination_airport\").alias(\"dst\")\n",
    "    )\n",
    "\n",
    "    # Create a GraphFrame\n",
    "    graph = GraphFrame(vertices, edges)\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run the connectedComponents algorithm\n",
    "    result = graph.connectedComponents()\n",
    "\n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "\n",
    "    # 4. Number of connected components\n",
    "    num_components = result.select(\"component\").distinct().count()\n",
    "\n",
    "    # 5. Size of each connected component\n",
    "    component_sizes = result.groupBy(\"component\").count().withColumnRenamed(\"count\", \"size\")\n",
    "\n",
    "    # 6. Airports in the largest connected component\n",
    "    largest_component_id = component_sizes.orderBy(F.desc(\"size\")).first()[\"component\"]\n",
    "    largest_component_airports = result.filter(F.col(\"component\") == largest_component_id).select(\"id\").collect()\n",
    "\n",
    "    # Print the execution time\n",
    "    print(f\"Execution Time (GraphFrames): {end_time - start_time} seconds\")\n",
    "\n",
    "    # Output the results\n",
    "    print(f\"Number of connected components: {num_components}\")\n",
    "    print(f\"Size of each connected component:\")\n",
    "    component_sizes.show()\n",
    "    \n",
    "    print(f\"Airports in the largest connected component:\")\n",
    "    for airport in largest_component_airports:\n",
    "        print(airport[\"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o63.loadClass.\n: java.lang.ClassNotFoundException: org.graphframes.GraphFramePythonAPI\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAirports2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Call the function with your DataFrame\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mfind_connected_components\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 31\u001b[0m, in \u001b[0;36mfind_connected_components\u001b[1;34m(flight_network_df, start_date, end_date)\u001b[0m\n\u001b[0;32m     25\u001b[0m edges \u001b[38;5;241m=\u001b[39m filtered_flights\u001b[38;5;241m.\u001b[39mselect(\n\u001b[0;32m     26\u001b[0m     F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOrigin_airport\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     27\u001b[0m     F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDestination_airport\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdst\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Create a GraphFrame\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mGraphFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvertices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Start the timer\u001b[39;00m\n\u001b[0;32m     34\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\graphframes\\graphframe.py:65\u001b[0m, in \u001b[0;36mGraphFrame.__init__\u001b[1;34m(self, v, e)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sqlContext \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39msql_ctx\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sqlContext\u001b[38;5;241m.\u001b[39m_sc\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm_gf_api \u001b[38;5;241m=\u001b[39m \u001b[43m_java_api\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mID \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm_gf_api\u001b[38;5;241m.\u001b[39mID()\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSRC \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm_gf_api\u001b[38;5;241m.\u001b[39mSRC()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\graphframes\\graphframe.py:38\u001b[0m, in \u001b[0;36m_java_api\u001b[1;34m(jsc)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_java_api\u001b[39m(jsc):\n\u001b[0;32m     37\u001b[0m     javaClassName \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.graphframes.GraphFramePythonAPI\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mThread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrentThread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetContextClassLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjavaClassName\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;241m.\u001b[39mnewInstance()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o63.loadClass.\n: java.lang.ClassNotFoundException: org.graphframes.GraphFramePythonAPI\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# Assuming you have a DataFrame called `flight_network_df` with 'origin', 'destination', and 'date' columns\n",
    "# Define your start and end dates (in 'yyyy-MM-dd' format)\n",
    "start_date = '1990-01-01'\n",
    "end_date = '1990-12-31'\n",
    "\n",
    "df = spark.read.csv(\"Airports2.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Call the function with your DataFrame\n",
    "find_connected_components(df, start_date, end_date)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
